{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Advance - 2 : Assignment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1. Explain the properties of the F-distribution.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of the F-Distribution\n",
    "\n",
    "The F-distribution is a continuous probability distribution commonly used in hypothesis testing, especially in ANOVA, regression analysis, and variance comparison. Below are the key properties of the F-distribution:\n",
    "\n",
    "1. Non-negative Values\n",
    "   - The F-distribution only takes on values from \\(0\\) to \\(+\\infty\\) since it’s based on the ratio of two variances (which are always positive).\n",
    "\n",
    "2. Right-Skewed\n",
    "   - The F-distribution is positively skewed, especially with smaller sample sizes. It becomes more symmetric as the degrees of freedom increase.\n",
    "\n",
    "3. Defined by Two Degrees of Freedom\n",
    "    - The shape of the F-distribution is determined by two degrees of freedom:\n",
    "\n",
    "   - **\\(df_1\\)** (numerator degrees of freedom): associated with the variance in the numerator.\n",
    "\n",
    "   - **\\(df_2\\)** (denominator degrees of freedom): associated with the variance in the denominator.\n",
    "\n",
    "4. Mean\n",
    "   - The mean exists when \\(df_2 > 2\\) and is calculated as: (Mean) = (df_2)/(df_2 - 2)\n",
    "\n",
    "5. Variance\n",
    "    - The variance exists when \\(df_2 > 4\\) and decreases as degrees of freedom increase, indicating less dispersion in F-values for larger sample sizes.\n",
    "\n",
    "6. Right-tail Use in Testing\n",
    "    - In hypothesis testing, the F-distribution is primarily used in the right tail. For example, in ANOVA, large F-values in the right tail suggest that the group means may not be equal, indicating a significant result.\n",
    "\n",
    "7. Derived from Chi-Square Ratios\n",
    "    - The F-distribution is derived from the ratio of two chi-square distributions, each divided by its degrees of freedom:\n",
    "\n",
    "\n",
    "These properties make the F-distribution ideal for comparing variances and testing for group differences across multiple groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-distribution is used in several types of statistical tests, primarily where comparisons of variances or ratios of variances are relevant. Here’s a look at key tests that rely on the F-distribution and why it is appropriate for each:\n",
    "\n",
    "**1. Analysis of Variance (ANOVA):**\n",
    "\n",
    " - Purpose: Used to determine if there are significant differences between the means of three or more groups.\n",
    "\n",
    " - Why F-distribution: In ANOVA, the F-distribution is used to compare the variance between groups to the variance within groups. A large F-value suggests that the group means are not all equal, making the F-distribution ideal for capturing these differences.\n",
    "\n",
    "**2. Regression Analysis :**\n",
    "\n",
    "\n",
    " - Purpose: Used to test the overall significance of a regression model and the impact of individual predictors.\n",
    "\n",
    " - Why F-distribution: In multiple regression, the F-distribution is used to evaluate the ratio of explained variance to unexplained variance. This helps determine if the model as a whole provides a better fit than a model with no predictors .\n",
    "\n",
    "**3. Testing Equality of Variances (F-test) :**\n",
    "\n",
    " - Purpose: Tests whether two populations have equal variances.\n",
    " \n",
    " - Why F-distribution: The F-distribution directly compares the ratio of variances from two independent samples. A significant F-value indicates that the variances differ, making it appropriate for testing homogeneity of variances.\n",
    "\n",
    "**4. Two-Way ANOVA :**\n",
    " - Purpose: Examines the impact of two different factors and their interaction on a dependent variable.\n",
    " \n",
    " - Why F-distribution: The F-distribution allows for testing of main effects (individual factors) and interaction effects, providing a comprehensive understanding of how each factor and their combination affect the outcome.\n",
    "\n",
    "**Why the F-Distribution is Appropriate -**\n",
    "The F-distribution is appropriate for these tests because it naturally arises from the ratio of variances, making it well-suited to compare variability across groups or test the significance of model effects. Its sensitivity to differences in variances allows it to indicate when observed group differences are statistically significant rather than due to chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met to ensure the validity of the test results:\n",
    "\n",
    "1. **Normality of Both Populations:**\n",
    "   - Each population from which the samples are drawn should follow a normal distribution. The F-test is sensitive to deviations from normality, so violations of this assumption may lead to inaccurate results.\n",
    "\n",
    "2. **Independence of Samples:**\n",
    "   - The samples must be independently selected from each population. This means that the measurements in one sample do not influence the measurements in the other sample.\n",
    "\n",
    "3. **Random Sampling:**\n",
    "   - Each sample should be randomly drawn from its respective population to ensure that it represents the broader population accurately.\n",
    "\n",
    "4. **Scale of Measurement:**\n",
    "   - The data should be measured on an interval or ratio scale, as the test relies on quantitative data with meaningful variances.\n",
    "\n",
    "5. **Positive Variances:**\n",
    "   - Variances must be positive since the F-statistic involves a ratio of variances, which are inherently non-negative.\n",
    "\n",
    "If these assumptions are met, the F-test can accurately assess whether there is a statistically significant difference in variances between the two populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4. What is the purpose of ANOVA, and how does it differ from a t-test?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of **ANOVA (Analysis of Variance)** is to determine whether there are statistically significant differences among the means of three or more groups. ANOVA is widely used in experimental design and data analysis to assess if at least one group mean differs significantly from others, which may indicate meaningful group effects or treatment impacts.\n",
    "\n",
    "Key Differences Between ANOVA and the t-test\n",
    "\n",
    "1. **Number of Groups Compared:**\n",
    "\n",
    "   - t-test: Compares the means of two groups.\n",
    "\n",
    "   - ANOVA: Compares the means of three or more groups simultaneously.\n",
    "   \n",
    "2. **Control of Type I Error:**\n",
    "\n",
    "   - t-test: Conducting multiple t-tests increases the risk of Type I error (false positives) since each test has its own probability of error.\n",
    "\n",
    "   - ANOVA: Controls Type I error by testing multiple group differences in a single test, ensuring the overall error rate remains at the chosen significance level (e.g., 5%).\n",
    "\n",
    "3. **Hypotheses Tested:**\n",
    "\n",
    "   - t-test: Tests whether the means of two groups are significantly different.\n",
    "\n",
    "   - ANOVA: Tests whether there is a significant difference in means among three or more groups. ANOVA’s null hypothesis states that all group means are equal, while the alternative hypothesis is that at least one group mean differs.\n",
    "\n",
    "4. **Applications:**\n",
    "\n",
    "   - t-test: Best for simple comparisons, such as testing the effect of a binary treatment.\n",
    "   \n",
    "   - ANOVA: Useful in multi-group experiments and complex designs, such as testing different doses of a drug or comparing outcomes across multiple categories.\n",
    "\n",
    "In summary, while a t-test is suitable for comparing two groups, ANOVA is better suited for situations involving three or more groups, allowing researchers to identify significant differences across groups while controlling for error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a **one-way ANOVA** instead of multiple t-tests when comparing the means of more than two groups to determine if there’s a statistically significant difference among them. Here’s why this approach is preferred:\n",
    "\n",
    "When to Use a One-Way ANOVA\n",
    "\n",
    "A one-way ANOVA is suitable when:\n",
    "\n",
    "1. **Three or More Groups**: You have three or more independent groups (e.g., different treatments, categories, or time points).\n",
    "\n",
    "2. **Testing for Any Difference**: You want to check if at least one group mean is different from the others.\n",
    "\n",
    "Why Use a One-Way ANOVA Instead of Multiple t-Tests?\n",
    "\n",
    "1. **Controls Type I Error**:\n",
    "\n",
    "   - Each t-test has a probability (e.g., 5%) of falsely finding a significant result (Type I error). Performing multiple t-tests increases this cumulative error rate. For example, comparing three groups requires three pairwise tests, which increases the overall chance of a false positive.\n",
    "\n",
    "   - One-way ANOVA maintains a consistent overall error rate (e.g., 5%) by testing all groups simultaneously, reducing the risk of false positives.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - A one-way ANOVA runs a single test to evaluate all groups, making it more efficient and less time-consuming than multiple t-tests, especially with large datasets.\n",
    "\n",
    "3. **Clear Interpretation**:\n",
    "   - One-way ANOVA provides a single test result that shows if there are any significant differences among groups. If significant, post-hoc tests can specify which groups differ, providing a structured and interpretable approach.\n",
    "\n",
    "In summary, one-way ANOVA is ideal when comparing three or more groups because it avoids inflated error rates, improves efficiency, and gives a clear indication of any group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In ANOVA, we’re trying to see if different groups have different averages (means). To do this, we look at the variability in our data, which is a measure of how spread out the values are.\n",
    "\n",
    "**Step 1: Total Variance :**\n",
    "\n",
    "First, we calculate the total variance in all our data. This tells us how spread out all the data points are from the overall average (the average of all groups combined). \n",
    "\n",
    "Then, we split this total variance into two parts:\n",
    "\n",
    "1. **Between-group variance** – How much do the averages of each group differ from each other?\n",
    "\n",
    "2. **Within-group variance** – How much do the individual data points in each group differ from their own group’s average?\n",
    "\n",
    "**Step 2: Between-Group Variance:**\n",
    "\n",
    "This part tells us how different the group averages are from the overall average. If the group averages are far from the overall average, then the between-group variance will be large, suggesting that the groups might be different from each other.\n",
    "\n",
    "**Step 3: Within-Group Variance:**\n",
    "\n",
    "This part tells us how much the values within each group differ from their own group’s average. If the values in each group are close to their group’s average, then the within-group variance will be small. If there’s a lot of spread within the groups, this variance will be large.\n",
    "\n",
    "**Step 4: The F-Statistic :**\n",
    "\n",
    "Now, we calculate something called the F-statistic, which is just a ratio:\n",
    "\n",
    "F = (between-group variance)/(within-group variance)\n",
    "\n",
    "\n",
    "- If the between-group variance is much bigger than the within-group variance, it suggests that the groups are quite different from each other, and the F-statistic will be large.\n",
    "\n",
    "- If the between-group variance is about the same as the within-group variance, it suggests that the groups are not very different, and the F-statistic will be close to 1.\n",
    "\n",
    "Final Step: Checking Significance\n",
    "\n",
    "We compare the F-statistic to a cutoff value to decide if the differences between group means are statistically significant. If the F-statistic is big enough, we conclude that at least one group is different from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical (frequentist) approach to ANOVA and the Bayesian approach both aim to assess differences between group means, but they differ fundamentally in how they treat uncertainty, estimate parameters, and perform hypothesis testing. Here’s a comparison of the key differences:\n",
    "\n",
    "1. **Handling Uncertainty**\n",
    "\n",
    "- Frequentist Approach:\n",
    "\n",
    "  - Uncertainty is managed using confidence intervals and p-values. \n",
    "\n",
    "  - The frequentist method assumes that parameters are fixed, unknown values. Uncertainty arises from the sampling process—different samples yield different estimates.\n",
    "\n",
    "  - The p-value indicates the probability of observing data as extreme as, or more extreme than, what we observed if there were truly no group differences (null hypothesis is true).\n",
    "\n",
    "- Bayesian Approach:\n",
    "\n",
    "  - Uncertainty is directly incorporated by assigning probability distributions to parameters, reflecting our beliefs about them before and after seeing the data.\n",
    "\n",
    "  - The Bayesian approach combines prior distributions (representing beliefs before data) with the observed data to create posterior distributions (updated beliefs after data).\n",
    "\n",
    "  - The posterior distribution captures uncertainty about parameters more fully by showing the range and probability of different parameter values.\n",
    "\n",
    "2. **Parameter Estimation**\n",
    "\n",
    "- Frequentist Approach:\n",
    "\n",
    "  - Parameters are estimated using methods like least squares. These estimates (such as group means) are treated as fixed points derived from the data.\n",
    "\n",
    "  - There are no probability distributions for parameters; instead, the variability in estimates across samples is represented by confidence intervals.\n",
    "\n",
    "- Bayesian Approach:\n",
    "\n",
    "  - Parameters are estimated as posterior distributions rather than single points.\n",
    "\n",
    "  - The result is a distribution of possible values for each parameter, which directly expresses both the estimated value and its uncertainty.\n",
    "\n",
    "  - Priors play a key role here, influencing the posterior distributions. The choice of prior can reflect prior knowledge or remain non-informative (to minimize influence).\n",
    "\n",
    "3. **Hypothesis Testing**\n",
    "\n",
    "- Frequentist Approach:\n",
    "\n",
    "  - Hypothesis testing in ANOVA is done using an F-test, which calculates an F-statistic and a p-value.\n",
    "\n",
    "  - The p-value is compared to a significance level (e.g., 0.05). If the p-value is below this level, we reject the null hypothesis, concluding that at least one group mean is different.\n",
    "\n",
    "  - This approach doesn't provide the probability that a hypothesis is true; rather, it tests whether the data are consistent with the null hypothesis.\n",
    "\n",
    "- Bayesian Approach:\n",
    "\n",
    "  - Hypothesis testing is done by comparing posterior probabilities. Bayesian ANOVA may compare different models (e.g., the null model vs. alternative models with different group means) and calculate Bayes factors.\n",
    "\n",
    "  - A Bayes factor quantifies the strength of evidence for one model over another, providing a direct probability ratio of the models given the data.\n",
    "\n",
    "  - Bayesian methods allow us to express the probability that one hypothesis is more likely than another, based on the data and the priors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 8. Question: You have two sets of data representing the incomes of two different professions:\n",
    "- Profession A: [48, 52, 55, 60, 62]\n",
    "- Profession B: [45, 50, 55, 52, 47]\n",
    "\n",
    "Perform an F-test to determine if the variances of the two professions' \n",
    "incomes are equal. What are your conclusions based on the F-test? \n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data. \n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Profession A: 32.8\n",
      "Variance of Profession B: 15.7\n",
      "F-statistic: 2.089171974522293\n",
      "p-value: 0.49304859900533904\n",
      "The variances of the two professions' incomes are not significantly different.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f\n",
    "import numpy as np\n",
    "\n",
    "# Data for Profession A and B\n",
    "prof_a = [48, 52, 55, 60, 62]\n",
    "prof_b = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Step 1: Calculate the sample variances of each dataset\n",
    "var_a = np.var(prof_a, ddof=1)\n",
    "var_b = np.var(prof_b, ddof=1)\n",
    "\n",
    "# Step 2: Calculate the F-statistic\n",
    "# F-statistic is the ratio of the larger variance to the smaller variance\n",
    "f_statistic = var_a / var_b if var_a > var_b else var_b / var_a\n",
    "\n",
    "# Step 3: Degrees of freedom\n",
    "dof1 = len(prof_a) - 1\n",
    "dof2 = len(prof_b) - 1\n",
    "\n",
    "# Step 4: Calculate the two-tailed p-value\n",
    "p_value = 2 * min(f.cdf(f_statistic, dof1, dof2), 1 - f.cdf(f_statistic, dof1, dof2))\n",
    "\n",
    "# Output the results with interpretation\n",
    "print(\"Variance of Profession A:\", var_a)\n",
    "print(\"Variance of Profession B:\", var_b)\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpretation based on significance level (alpha)\n",
    "alpha = 0.05\n",
    "if p_value > alpha:\n",
    "    print(\"The variances of the two professions' incomes are not significantly different.\")\n",
    "else:\n",
    "    print(\"The variances of the two professions' incomes are significantly different.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data :\n",
    "     - Region A: [160, 162, 165, 158, 164]\n",
    "     - Region B: [172, 175, 170, 168, 174]\n",
    "     - Region C: [180, 182, 179, 185, 183]\n",
    "\n",
    "- Task: Write Python code to perform the one-way ANOVA and interpret the results \n",
    "- Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.8706641879370266e-07\n",
      "There is a significant difference in average heights between the three regions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nInterpretation:\\n- If the p-value is less than the significance level (commonly 0.05), we reject the null hypothesis and conclude that there is a statistically significant difference between the average heights of the regions.\\n- If the p-value is greater than 0.05, we fail to reject the null hypothesis, implying that any observed differences in average heights are likely due to chance.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Define the data for the three regions\n",
    "region_a = [160, 162, 165, 158, 164]\n",
    "region_b = [172, 175, 170, 168, 174]\n",
    "region_c = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform the one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Set the significance level (alpha)\n",
    "alpha = 0.05\n",
    " \n",
    "# Interpret the results\n",
    "if p_value > alpha:\n",
    "    print(\"There is no significant difference in average heights between the three regions.\")\n",
    "else:\n",
    "    print(\"There is a significant difference in average heights between the three regions.\")\n",
    "\n",
    "'''\n",
    "Interpretation:\n",
    "- If the p-value is less than the significance level (commonly 0.05), we reject the null hypothesis and conclude that there is a statistically significant difference between the average heights of the regions.\n",
    "- If the p-value is greater than 0.05, we fail to reject the null hypothesis, implying that any observed differences in average heights are likely due to chance.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
